---
title: "Comparative Track Analysis: AI Jazz & Jimi Hendrix"
author: "Jasper Valk"
date: "`r Sys.Date()`"
output: 
  flexdashboard::flex_dashboard:
    storyboard: true
    theme:
      version: 4
      bg: "#fefae0"
      fg: "#0c1618"
      primary: "#243e36"
      navbar-bg: "#7ca982"
      base_font: 
        google: Prompt
      heading_font:
        google: Sen
      code_font:
        google: 
          family: JetBrains Mono
          local: false
    vertical_layout: fill
---

```{r setup, include=FALSE}
library(flexdashboard)
library(tidyverse)
library(plotly)
library(gt)
library(corrplot)
library(heatmaply)
library(tidymodels)
library(ggdendro)
library(htmltools)
source("compmus.R")
compmus2025 <- read_csv("compmus2025.csv")

# Define templates
major_chord <- c(1,0,0,0,1,0,0,1,0,0,0,0)
minor_chord <- c(1,0,0,1,0,0,0,1,0,0,0,0)
seventh_chord <- c(1,0,0,0,1,0,0,1,0,0,1,0)

major_key <- c(6.35, 2.23, 3.48, 2.33, 4.38, 4.09, 2.52, 5.19, 2.39, 3.66, 2.29, 2.88)
minor_key <- c(6.33, 2.68, 3.52, 5.38, 2.60, 3.53, 2.54, 4.75, 3.98, 2.69, 3.34, 3.17)

circshift <- function(x, n) {
  if (n == 0) return(x)
  c(tail(x, -n), head(x, n))
}

chord_templates <- tribble(
  ~name, ~template,
  "C:maj", circshift(major_chord, 0),
  "C#:maj", circshift(major_chord, 1),
  "D:maj", circshift(major_chord, 2),
  "D#:maj", circshift(major_chord, 3),
  "E:maj", circshift(major_chord, 4),
  "F:maj", circshift(major_chord, 5),
  "F#:maj", circshift(major_chord, 6),
  "G:maj", circshift(major_chord, 7),
  "G#:maj", circshift(major_chord, 8),
  "A:maj", circshift(major_chord, 9),
  "A#:maj", circshift(major_chord, 10),
  "B:maj", circshift(major_chord, 11)
)

key_templates <- tribble(
  ~name, ~template,
  "C:maj", circshift(major_key, 0),
  "C#:maj", circshift(major_key, 1),
  "D:maj", circshift(major_key, 2),
  "D#:maj", circshift(major_key, 3),
  "E:maj", circshift(major_key, 4),
  "F:maj", circshift(major_key, 5),
  "F#:maj", circshift(major_key, 6),
  "G:maj", circshift(major_key, 7),
  "G#:maj", circshift(major_key, 8),
  "A:maj", circshift(major_key, 9),
  "A#:maj", circshift(major_key, 10),
  "B:maj", circshift(major_key, 11)
)

```

Introduction {.storyboard}
====================================================

### Overview

This dashboard presents a musicological comparison between two unique tracks:
- **Track 1**: An AI-generated jazz piece
- **Track 2**: *Hey Joe* by Jimi Hendrix (a live-recorded, human-composed classic)

By analyzing multiple musical attributes like chroma, timbre, rhythm, tonality, and clustering, we are uncovering contrasts in structure and musical expression between synthetic and human creativity.

```{r, echo=FALSE}
tags$div(
  tags$h4("Track 1: AI-Generated Jazz"),
  tags$p("This song was created with the use of a AI tool named 'Suno' on suno.com. The song is created with the prompt 'create a jazz song about the impact of AI on the music industry that is about 2 minutes long'. The song is about what is named in the prompt. I chose jazz because it is largely improvisational, relying on spontaneity and creativity in the moment. This makes it especially interesting for an AI to generate. I wanted to challenge the system to replicate the fluidity and expressive nuance that is in jazz music."),
  tags$h4("Track 2: 'Hey Joe' by Jimi Hendrix"),
  tags$p("The second song is 'Hey Joe' from Jimi Hendrix. It is downloaded from the site; https://emp3juice.la/. I Chose this song because I'm a big fan of Jimi Hendrix and in my opinion, the best guitarist that has ever lived. Because I'm such a big fan of his, I really wanted to know more about his music by analyzing it during this course, and that is what I did")
)

```


```{r, results='asis', echo=FALSE}
cat('
<h4>Listen: AI Jazz</h4>
<audio controls>
  <source src="jasper-v-1.mp3" type="audio/mp3">
  Your browser does not support the audio element.
</audio>
')
```

```{r,  results='asis', echo=FALSE}
cat('
<h4>Listen: Hey Joe (Hendrix)</h4>
<audio controls>
  <source src="jasper-v-2.mp3" type="audio/mp3">
  Your browser does not support the audio element.
</audio>
')
```


Track-Level Summary {.storyboard}
====================================================

### Feature Correlation Matrix
```{r}
corrplot(cor(compmus2025[sapply(compmus2025, is.numeric)], use = "pairwise.complete.obs"), method = 'number')
```

***
All analyses assume that the JSON files are correctly parsed and that the AI-generated track follows Western tonal systems. Novelty plots assume consistent loudness normalization. These assumptions may affect how results are interpreted, especially with unconventional AI music structures.

### Arousal vs Valence (with Highlighted Tracks)
```{r}
my_tracks <- compmus2025 |> filter(filename %in% c("jasper-v-1", "jasper-v-2"))
compmus2025 |> 
  ggplot(aes(x = arousal, y = valence)) + 
  geom_point(alpha = 0.3) + 
  geom_point(data = my_tracks, color = 'red', size = 3) + 
  theme_minimal() + 
  labs(title = "Arousal vs Valence with My Tracks Highlighted")
```

### Danceability vs Instrumentalness
```{r}
ggplot(compmus2025, aes(x = danceability, y = instrumentalness)) + 
  geom_point(alpha = 0.5) + 
  theme_minimal() + 
  labs(title = "Danceability vs Instrumentalness")
```

AI Jazz (Track 1) {.storyboard}
====================================================

### Chromagram
```{r}
"features/jasper-v-1.json" |> compmus_chroma(norm = "identity") |>
  ggplot(aes(x = time, y = pc, fill = value)) +
  geom_raster() +
  scale_y_continuous(breaks = 0:11, labels = LETTERS[1:12]) +
  scale_fill_viridis_c(guide = "none") +
  theme_classic()
```

### Chordogram – AI Jazz
```{r}
"features/jasper-v-1.json" |> 
  compmus_chroma(norm = "identity") |> 
  compmus_match_pitch_templates(chord_templates, norm = "identity", distance = "cosine") |> 
  ggplot(aes(x = time, y = name, fill = d)) +
  geom_raster() +
  scale_fill_viridis_c(guide = "none") +
  labs(x = "Time (s)", y = "Chord", title = "Chordogram – AI Jazz") +
  theme_classic()

```


### Spectral Novelty
```{r}
"features/jasper-v-1.json" |> compmus_spectral_novelty() |> 
  ggplot(aes(t, novelty)) +
  geom_line(color = "#FFDE59") +
  theme_minimal() +
  labs(x = "Time (s)", y = "Spectral Novelty")
```

### Chroma Self-Similarity
```{r}
"features/jasper-v-1.json" |> 
  compmus_chroma(norm = "identity") |> 
  compmus_self_similarity(feature = pc, distance = "cosine") |> 
  ggplot(aes(x = xtime, y = ytime, fill = d)) +
  geom_raster() +
  scale_fill_viridis_c(guide = "none") +
  labs(x = "Time (s)", y = NULL, title = "Chroma-Based Self-Similarity – AI Jazz") +
  theme_classic()

```


### Tempogram
```{r}
"features/jasper-v-1.json" |> 
  compmus_tempogram(window_size = 32, hop_size = 5, cyclic = TRUE) |> 
  ggplot(aes(x = time, y = bpm, fill = power)) +
  geom_raster() +
  scale_fill_viridis_c(guide = "none") +
  labs(x = "Time (s)", y = "Tempo (BPM)", title = "Tempogram – AI Jazz (Reduced Resolution)") +
  theme_classic()

```

### Timbre Self-Similarity
```{r}
"features/jasper-v-1.json" |> 
  compmus_mfccs(norm = "identity") |> 
  compmus_self_similarity(feature = mfcc, distance = "euclidean") |> 
  ggplot(aes(x = xtime, y = ytime, fill = d)) +
  geom_raster() +
  scale_fill_viridis_c(guide = "none") +
  labs(x = "Time (s)", y = NULL) +
  theme_classic()
```

Hey Joe – Hendrix (Track 2) {.storyboard}
====================================================
### Chromagram
```{r}
"features/jasper-v-2.json" |> compmus_chroma(norm = "identity") |>
  ggplot(aes(x = time, y = pc, fill = value)) +
  geom_raster() +
  scale_y_continuous(breaks = 0:11, labels = LETTERS[1:12]) +
  scale_fill_viridis_c(guide = "none") +
  theme_classic()
```

### Cepstrogram – Hendrix

```{r}
"features/jasper-v-2.json" |> 
  compmus_mfccs(norm = "identity") |> 
  ggplot(aes(x = time, y = mfcc, fill = value)) + 
  geom_raster() +
  scale_fill_viridis_c(guide = "none") +
  labs(x = "Time (s)", y = "MFCC Coefficient", title = "Cepstrogram – Hendrix") +
  theme_classic()
```


### Energy Novelty
```{r}
"features/jasper-v-2.json" |> compmus_energy_novelty() |> 
  ggplot(aes(t, novelty)) +
  geom_line(color = "#79F9B7") +
  theme_minimal() +
  labs(x = "Time (s)", y = "Energy Novelty")
```

### Keygram
```{r}
"features/jasper-v-2.json" |> 
  compmus_chroma(norm = "euclidean") |> 
  compmus_match_pitch_templates(key_templates, norm = "euclidean", distance = "angular") |> 
  ggplot(aes(x = time, y = name, fill = d)) +
  geom_raster() +
  scale_fill_viridis_c(guide = "none") +
  labs(x = "Time (s)", y = "Key") +
  theme_classic()
```

### Chordogram – Hendrix
```{r}

"features/jasper-v-2.json" |> 
  compmus_chroma(norm = "identity") |> 
  compmus_match_pitch_templates(chord_templates, norm = "identity", distance = "cosine") |> 
  ggplot(aes(x = time, y = name, fill = d)) +
  geom_raster() +
  scale_fill_viridis_c(guide = "none") +
  labs(x = "Time (s)", y = "Chord", title = "Chordogram – Hendrix") +
  theme_classic()

```

### Chroma Self-Similarity
```{r}
"features/jasper-v-2.json" |> 
  compmus_chroma(norm = "identity") |> 
  compmus_self_similarity(feature = pc, distance = "cosine") |> 
  ggplot(aes(x = xtime, y = ytime, fill = d)) +
  geom_raster() +
  scale_fill_viridis_c(guide = "none") +
  labs(x = "Time (s)", y = NULL) +
  theme_classic()
```

### Tempogram – Hendrix
```{r}
"features/jasper-v-2.json" |> 
  compmus_tempogram(window_size = 32, hop_size = 5, cyclic = TRUE) |> 
  ggplot(aes(x = time, y = bpm, fill = power)) +
  geom_raster() +
  scale_fill_viridis_c(guide = "none") +
  labs(x = "Time (s)", y = "Tempo (BPM)", title = "Tempogram – Hendrix") +
  theme_classic()
```

Comparative Clustering {.storyboard}
====================================================

### Dendrogram (Complete Linkage)
```{r}
cluster_juice <- recipe(filename ~ arousal + danceability + instrumentalness + tempo + valence, data = compmus2025) |>
  step_center(all_predictors()) |>
  step_scale(all_predictors()) |>
  prep(compmus2025) |> juice() |> column_to_rownames("filename")

compmus_dist <- dist(cluster_juice, method = "euclidean")
compmus_dist |> hclust(method = "complete") |> dendro_data() |> ggdendrogram()
```

### Heatmap of Feature Profiles
```{r}
heatmaply(
  cluster_juice,
  hclustfun = hclust,
  hclust_method = "complete",
  dist_method = "euclidean"
)
```

Conclusion {.storyboard}
====================================================

### Final Thoughts
- **AI Jazz** displays smoother energy flow and tonal center variety. Its novelty and tempo are more uniform.
- **Hendrix** features dramatic energy shifts and harmonic richness. Self-similarity matrices show strong structural recurrence.
- Clustering confirms they belong to distinct musical regions in the corpus.

The contrast between the AI-generated jazz track and Hey Joe by Jimi Hendrix paints a clear picture of two very different musical approaches—one shaped by algorithmic predictability and the other by human expressiveness. The AI Jazz track displays a smooth, continuous flow with relatively uniform energy and a stable tonal center. The novelty functions reveal a lack of sharp transitions, which suggests that the structure is intentionally restrained, likely following a learned pattern of what “relaxed” jazz should sound like. This consistency contributes to a calm and ambient vibe, but also points to a limited dynamic range.

On the other hand, Hendrix’s Hey Joe presents a more organic and expressive musical landscape. The energy novelty curve features dramatic peaks and valleys, aligning with moments of instrumental intensity and vocal delivery. The keygram shows shifts that suggest real-time modulation or expressive deviations from a strict tonal center. In the chroma and timbre self-similarity matrices, we see pronounced diagonal structures, highlighting recurring themes and motifs—hallmarks of intentional musical phrasing and narrative.

When placed in the broader context of the class corpus, clustering clearly places these tracks in separate sonic neighborhoods. The AI jazz track clusters near instrumental, lower-arousal tracks with moderate tempo and high smoothness, while Hey Joe stands apart—its position influenced by a more dynamic interplay of arousal, valence, and tempo. It resonates more closely with expressive human performances that vary across multiple musical dimensions.

Overall, this analysis reveals how AI can effectively mimic stylistic elements—structure, tone, even genre-specific features—but still falls short of delivering the nuanced spontaneity that comes with human musicianship. Hendrix’s track reminds us that imperfections, tension, and surprise are not flaws in music—they’re often the most memorable parts.


Cross-Track Feature Comparison {.storyboard}
====================================================

### Interactive Valence & Tempo Comparison
```{r}
interactive_plot <- compmus2025 |> 
  mutate(highlight = case_when(
    filename == "jasper-v-1" ~ "AI Jazz",
    filename == "jasper-v-2" ~ "Hendrix",
    TRUE ~ "Other"
  )) |> 
  ggplot(aes(x = tempo, y = valence, color = highlight, size = danceability)) +
  geom_point(alpha = 0.6) +
  scale_color_manual(values = c("AI Jazz" = "#38BDF8", "Hendrix" = "#FACC15", "Other" = "#94A3B8")) +
  theme_minimal() +
  labs(
    title = "Valence vs Tempo (Highlighted Tracks)",
    x = "Tempo (BPM)",
    y = "Valence",
    color = "Track Type",
    size = "Danceability"
  )

ggplotly(interactive_plot)
```

***
Interestingly, although Hendrix is known for expressive playing, its valence score is quite neutral, suggesting the limitations of automated valence extraction in expressive genres.

### Combined Novelty Patterns
```{r}
energy_ai <- compmus_energy_novelty("features/jasper-v-1.json") |> mutate(track = "AI Jazz")
energy_hj <- compmus_energy_novelty("features/jasper-v-2.json") |> mutate(track = "Hendrix")

bind_rows(energy_ai, energy_hj) |> 
  ggplot(aes(x = t, y = novelty, color = track)) +
  geom_line(size = 1) +
  theme_minimal() +
  labs(title = "Energy Novelty Comparison", x = "Time (s)", y = "Novelty")
```


Track Similarity Summary {.storyboard}
====================================================

### Euclidean Distance Table (Top Matches)
```{r}
dist_matrix <- as.matrix(dist(cluster_juice))
top_matches <- dist_matrix[rownames(dist_matrix) %in% c("jasper-v-1", "jasper-v-2"), ] |> 
  as.data.frame() |> 
  rownames_to_column("track") |> 
  pivot_longer(-track, names_to = "compared_with", values_to = "distance") |>
  filter(compared_with != track) |> 
  group_by(track) |> 
  slice_min(distance, n = 5) |> 
  ungroup()

top_matches |> gt()
```

### Summary Table: Features of My Tracks
```{r}
compmus2025 |> 
  filter(filename %in% c("jasper-v-1", "jasper-v-2")) |> 
  select(filename, tempo, arousal, valence, danceability, instrumentalness) |> 
  gt() |> 
  tab_header(title = "Feature Summary: AI Jazz vs Hendrix")
```

Thank You {.storyboard}
====================================================

### Reflection
This portfolio demonstrates a cross-sectional analysis of an AI-composed jazz track and a live recording by Jimi Hendrix. It leverages advanced R-based tools for analyzing musical data and contributes to a broader conversation about creativity, structure, and the role of AI in music.

Using chroma-based self-similarity gave me new insight into how repetition manifests in both algorithmic and human compositions.

### Possible Future Work
- Add lyrical sentiment analysis for human tracks
- Use supervised models to predict genre or composer

---
*Prepared for the Computational Musicology 2025 final portfolio presentation.*

